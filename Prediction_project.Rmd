---
title: "Prediction_project"
author: "David"
date: "25 December 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Initialization: load packages and dataset

```{r}
library(ggplot2); library(caret); library(corrplot)
dataset <- read.csv("C:/Coursera/pml-training.csv", na.strings = c("NA",""))
validation_set <- read.csv("C:/Coursera/pml-testing.csv", na.strings = c("NA",""))
```

## We are now checking the structure of the data

```{r}
summary(dataset)
str(dataset)
ls(dataset)
```

# We can see that the database contains variable with NAs.
# We will have to remove it for our analysis.

```{r}
colSums(is.na(dataset))
dataset_cleaned <- dataset[ , colSums(is.na(dataset)) == 0]

dim(dataset)
dim(dataset_cleaned)
```
# Removing the variables without measures

```{r}
dataset_cleaned <- dataset_cleaned[,-c(1:7)]
```

## We split the dataset between Train set and Test set

```{r}
inTrain = createDataPartition(dataset_cleaned$classe, p = 3/4)[[1]]
training = dataset_cleaned[ inTrain,]
testing = dataset_cleaned[-inTrain,]
```

## We plot the class to see the repartition
```{r}
ggplot(training, aes(x = classe, fill = classe)) +
  geom_bar(stat='count', position='dodge') +  geom_label(stat='count', aes(label=..count..))
```

## We plot the variables to identify which are correlated
```{r}
ls(training)
CorMatrix_gyros <- cor(training[,15:26]) 
CorMatrix_magnet <- cor(training[,27:38])
corrplot(CorMatrix_gyros, method = "number")
corrplot(CorMatrix_magnet, method = "number")
```

## We are now training the models using cross validation 

# we repeated 5 fold 2 times to tune the algorithms parameters
```{r}
control <- trainControl(method='repeatedcv', number=5, repeats=2)
```

# We fit a Random Forest model using all the predictor that remain in our database
```{r}
mod_rf <- train(classe ~ ., data = training, trControl=control , method = "rf")
print(mod_rf)
```

# We fit a Generailized Boosting Machine model using all the predictor that remain in our database too
```{r}
mod_gbm <- train(classe ~ ., data = training, trControl=control , method = "gbm", verbose=FALSE)
print(mod_gbm)
```

## Prediction on the testing set:

# Using RF
```{r}
pred_rf <- predict(mod_rf, testing)
cfm_rf <- confusionMatrix(testing$classe, pred_rf)
cfm_rf$table
cfm_rf$overall["Accuracy"]
```

# Using GBM
```{r}
pred_gbm <- predict(mod_gbm, testing)
cfm_gbm <- confusionMatrix(testing$classe, pred_gbm)
cfm_gbm$table
cfm_gbm$overall["Accuracy"]
```

# The RF model is more accurate than GBM, therefore we will proceed with it for the final prediction.

## Conclusion 

# Prediction on the validation set
```{r}
Valid_pred_rf <- predict(mod_rf, newdata=validation_set)
```

# We concerve only the results
```{r}
Results <- cbind(validation_set, Pred=Valid_pred_rf)[,c(160,161)]
print(Results)
```
